{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext import data, datasets, vocab\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import random, tqdm, sys, math, gzip\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda:0'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    return device\n",
    "DEVICE = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d:int, d_k=int, d_v=int, heads=int):\n",
    "        super().__init__()\n",
    "        #\n",
    "        self.d = d\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.heads = heads        \n",
    "        #\n",
    "        self.to_K = nn.Linear(d, d_k * heads, bias=False)\n",
    "        self.to_Q = nn.Linear(d, d_k * heads, bias=False)\n",
    "        self.to_V = nn.Linear(d, d_v * heads, bias=False)\n",
    "        #\n",
    "        self.concat_heads = nn.Linear(heads * d_v, d_v)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \n",
    "        b, L, d = X.size()\n",
    "        assert self.d == d\n",
    "        #\n",
    "        Q = self.to_Q(X)\n",
    "        K = self.to_K(X)\n",
    "        V = self.to_V(X)\n",
    "        #\n",
    "        Q = Q.view((b, L, self.heads, self.d_k))\n",
    "        K = K.view((b, L, self.heads, self.d_k))\n",
    "        V = V.view((b, L, self.heads, self.d_v))\n",
    "        #\n",
    "        assert Q.shape == torch.Size((b, L, self.heads, self.d_k))\n",
    "        assert K.shape == torch.Size((b, L, self.heads, self.d_k))\n",
    "        assert V.shape == torch.Size((b, L, self.heads, self.d_v))\n",
    "        #\n",
    "        # reshape (b, L, h, d) to (b, h, L, d)\n",
    "        K = K.transpose(1, 2).contiguous().view(b * self.heads, L, self.d_k)\n",
    "        Q = Q.transpose(1, 2).contiguous().view(b * self.heads, L, self.d_k)\n",
    "        V = V.transpose(1, 2).contiguous().view(b * self.heads, L, self.d_v)\n",
    "        #\n",
    "        # scale stuff\n",
    "        # k^(1/4) * k^(1/4) = k^(1/2)\n",
    "        Q = Q / (self.d_k ** (1/4))\n",
    "        K = K / (self.d_k ** (1/4))\n",
    "        # calculate attention\n",
    "        Z = torch.bmm(Q, K.transpose(1, 2))\n",
    "        #\n",
    "        assert Z.size() == (b*self.heads, L, L)\n",
    "        #\n",
    "        A = F.softmax(Z, dim=2)\n",
    "        \n",
    "        # output\n",
    "        Y = torch.bmm(A, V).view(b, self.heads, L, self.d_v)\n",
    "        \n",
    "        Y = Y.transpose(1, 2).contiguous().view(b, L, self.heads * self.d_v)\n",
    "        Y = self.concat_heads(Y)\n",
    "        return Y, A\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d:int, heads:int, layer_width=8):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention = MultiHeadAttention(d, d, d, heads)\n",
    "        self.norm1 = nn.LayerNorm(d)\n",
    "        self.norm2 = nn.LayerNorm(d)\n",
    "    \n",
    "        self.mlp = nn.Sequential(\n",
    "              nn.Linear(d, layer_width * d),\n",
    "              nn.ReLU(),\n",
    "              nn.Linear(d * layer_width, d))\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y, _ = self.attention(X)\n",
    "        X = self.norm1(Y + X)\n",
    "        #\n",
    "        Y = self.mlp(X)\n",
    "        X = self.norm2(Y + X)\n",
    "        \n",
    "        return X\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d:int, heads:int, depth:int, seq_length:int, num_tokens:int, num_classes:int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_tokens = num_tokens\n",
    "        self.token_emb = nn.Embedding(num_tokens, d)\n",
    "        self.pos_emb = nn.Embedding(seq_length, d)\n",
    "\n",
    "        # The sequence of transformer blocks that does all the \n",
    "        # heavy lifting\n",
    "        tblocks = []\n",
    "        for i in range(depth):\n",
    "            tblocks.append(TransformerBlock(d=d, heads=heads))\n",
    "        self.tblocks = nn.Sequential(*tblocks)\n",
    "\n",
    "        # Maps the final output sequence to class logits\n",
    "        self.toprobs = nn.Linear(d, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: A (b, t) tensor of integer values representing \n",
    "                  words (in some predetermined vocabulary).\n",
    "        :return: A (b, c) tensor of log-probabilities over the \n",
    "                 classes (where c is the nr. of classes).\n",
    "        \"\"\"\n",
    "        # generate token embeddings\n",
    "        tokens = self.token_emb(x)\n",
    "        b, t, k = tokens.size()\n",
    "\n",
    "        # generate position embeddings\n",
    "        positions = torch.arange(t, device=DEVICE)\n",
    "        positions = self.pos_emb(positions)[None, :, :].expand(b, t, k)\n",
    "        \n",
    "        x = tokens + positions\n",
    "        x = self.tblocks(x)\n",
    "        \n",
    "        # Average-pool over the t dimension and project to class \n",
    "        # probabilities\n",
    "        x = self.toprobs(x.mean(dim=1))\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "clear_data = False\n",
    "vocab_size = 50_000\n",
    "batch_size = 16\n",
    "epochs = 100\n",
    "lr = 0.0001\n",
    "lr_warmup = 10_000\n",
    "d = 128\n",
    "max_length = 512 # max sequence length\n",
    "heads = 8\n",
    "depth = 8\n",
    "classes=2\n",
    "p_base = Path(\"/home/matthias/projects/Transformer\")\n",
    "p_checkpoints = p_base / \"data\" / \"checkpoints\"\n",
    "p_tb = p_base / \"data\" / \"tensorboard\"\n",
    "p_checkpoint = p_checkpoints / \"model_cp_{}.cp\"\n",
    "\n",
    "if clear_data:\n",
    "    if p_checkpoints.exists():\n",
    "        shutil.rmtree(p_checkpoints)\n",
    "    if p_tb.exists():\n",
    "        shutil.rmtree(p_tb)\n",
    "\n",
    "p_checkpoints.mkdir(exist_ok=True)\n",
    "p_tb.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW CONFIGS, MAYBE BETTER\n",
    "clear_data = False\n",
    "vocab_size = 50_000\n",
    "batch_size = 4\n",
    "epochs = 80\n",
    "lr = 0.0001\n",
    "lr_warmup = 10_000\n",
    "d = 128\n",
    "max_length = 512 # max sequence length\n",
    "heads = 8\n",
    "depth = 6\n",
    "classes=2\n",
    "p_base = Path(\"/home/matthias/projects/Transformer\")\n",
    "p_checkpoints = p_base / \"data\" / \"checkpoints\"\n",
    "p_tb = p_base / \"data\" / \"tensorboard\"\n",
    "p_checkpoint = p_checkpoints / \"model_cp_{}.cp\"\n",
    "\n",
    "if clear_data:\n",
    "    if p_checkpoints.exists():\n",
    "        shutil.rmtree(p_checkpoints)\n",
    "    if p_tb.exists():\n",
    "        shutil.rmtree(p_tb)\n",
    "\n",
    "p_checkpoints.mkdir(exist_ok=True)\n",
    "p_tb.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthias/anaconda3/envs/pt/lib/python3.8/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/home/matthias/anaconda3/envs/pt/lib/python3.8/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/home/matthias/anaconda3/envs/pt/lib/python3.8/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "TEXT = data.Field(lower=True, include_lengths=True, batch_first=True)\n",
    "LABEL = data.Field(sequential=False)\n",
    "\n",
    "tdata, _ = datasets.IMDB.splits(TEXT, LABEL)\n",
    "train, test = tdata.split(split_ratio=0.8)\n",
    "\n",
    "TEXT.build_vocab(train, max_size=vocab_size - 2) # - 2 to make space for <unk> and <pad>\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "train_iter, test_iter = data.BucketIterator.splits((train, test), batch_size=batch_size, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbw = SummaryWriter(log_dir=str(p_tb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(d=d, heads=heads, depth=depth, seq_length=max_length, num_tokens=vocab_size, num_classes=classes).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(lr=lr, params=model.parameters())\n",
    "sch = torch.optim.lr_scheduler.LambdaLR(opt, lambda i: min(i / (lr_warmup / batch_size), 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthias/anaconda3/envs/pt/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "100%|██████████| 1250/1250 [04:22<00:00,  4.76it/s]\n",
      "  0%|          | 0/1250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- \"test\" accuracy 0.683\n",
      "\n",
      " epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 125/1250 [00:26<04:08,  4.53it/s]"
     ]
    }
   ],
   "source": [
    "seen = 0\n",
    "for e in range(epochs):\n",
    "    print(f'\\n epoch {e}')\n",
    "    model.train(True)\n",
    "\n",
    "    for batch in tqdm.tqdm(train_iter):\n",
    "\n",
    "        opt.zero_grad()\n",
    "\n",
    "        input = batch.text[0]\n",
    "        label = batch.label - 1\n",
    "\n",
    "        if input.size(1) > max_length:\n",
    "            input = input[:, :max_length]\n",
    "        out = model(input)\n",
    "        loss = F.nll_loss(out, label)\n",
    "\n",
    "        loss.backward()\n",
    "        #\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        #\n",
    "        opt.step()\n",
    "        sch.step()\n",
    "        #\n",
    "        seen += input.size(0)\n",
    "        tbw.add_scalar('classification/train-loss', float(loss.item()), seen)\n",
    "        \n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.train(False)\n",
    "        tot, cor= 0.0, 0.0\n",
    "\n",
    "        for batch in test_iter:\n",
    "            input = batch.text[0]\n",
    "            label = batch.label - 1\n",
    "\n",
    "            if input.size(1) > max_length:\n",
    "                input = input[:, :max_length]\n",
    "            out = model(input).argmax(dim=1)\n",
    "\n",
    "            tot += float(input.size(0))\n",
    "            cor += float((label == out).sum().item())\n",
    "        acc = cor / tot\n",
    "        tbw.add_scalar('classification/acc', float(acc), e)\n",
    "        print(f'-- \"test\" accuracy {acc:.3}')\n",
    "    # save parameters\n",
    "    torch.save(model.state_dict(), str(p_checkpoint).format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load(path.format(e)))\n",
    "#model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
