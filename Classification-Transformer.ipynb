{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext import data, datasets, vocab\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import random, tqdm, sys, math, gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d:int, d_k=int, d_v=int):\n",
    "        super().__init__()\n",
    "        #\n",
    "        self.to_Q = nn.Linear(d, d_k, bias=False)\n",
    "        self.to_K = nn.Linear(d, d_k, bias=False)\n",
    "        self.to_V = nn.Linear(d, d_v, bias=False)\n",
    "    \n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.d = d\n",
    "\n",
    "    def forward(self, X):\n",
    "        \n",
    "        b, L, d = X.size()\n",
    "        \n",
    "        Q = self.to_Q(X)\n",
    "        K = self.to_K(X)\n",
    "        V = self.to_V(X)\n",
    "        \n",
    "        assert Q.shape == torch.Size((b, L, self.d_k))\n",
    "        assert K.shape == torch.Size((b, L, self.d_k))\n",
    "        assert V.shape == torch.Size((b, L, self.d_v))\n",
    "        \n",
    "        #Q = Q.view((b, L, self.d_k))\n",
    "        #K = K.view((b, L, self.d_k))\n",
    "        #V = V.view((b, L, self.d_v))\n",
    "        \n",
    "        # scale stuff\n",
    "        # k^(1/4) * k^(1/4) = k^(1/2)\n",
    "        Q = Q / self.d_k ** (1/4)\n",
    "        K = K / self.d_k ** (1/4)\n",
    "        \n",
    "        # calculate attention\n",
    "        Z = torch.bmm(Q, K.transpose(1, 2))\n",
    "        A = F.softmax(Z, dim=2) \n",
    "        \n",
    "        # output\n",
    "        Y = torch.bmm(A, V)\n",
    "        \n",
    "        assert Y.shape == torch.Size((b, L, self.d_v))\n",
    "        \n",
    "        return Y, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 2\n",
    "L = 10\n",
    "d = 4\n",
    "d_k = d\n",
    "d_v = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "X = torch.rand((b, L, d))\n",
    "#\n",
    "model = Attention(d=d, d_k=d_k, d_v=d_v)\n",
    "#\n",
    "Y, A = model(X)\n",
    "#\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Q.shape, K.shape, V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.view((b, L, d_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2**(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2**(1/4) * 2**(1/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
