{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext import data, datasets, vocab\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import random, tqdm, sys, math, gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda:0'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    return device\n",
    "DEVICE = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d:int, d_k=int, d_v=int):\n",
    "        super().__init__()\n",
    "        #\n",
    "        self.to_Q = nn.Linear(d, d_k, bias=False)\n",
    "        self.to_K = nn.Linear(d, d_k, bias=False)\n",
    "        self.to_V = nn.Linear(d, d_v, bias=False)\n",
    "    \n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.d = d\n",
    "\n",
    "    def forward(self, X):\n",
    "        \n",
    "        b, L, d = X.size()\n",
    "        \n",
    "        Q = self.to_Q(X)\n",
    "        K = self.to_K(X)\n",
    "        V = self.to_V(X)\n",
    "        \n",
    "        assert Q.shape == torch.Size((b, L, self.d_k))\n",
    "        assert K.shape == torch.Size((b, L, self.d_k))\n",
    "        assert V.shape == torch.Size((b, L, self.d_v))\n",
    "        \n",
    "        #Q = Q.view((b, L, self.d_k))\n",
    "        #K = K.view((b, L, self.d_k))\n",
    "        #V = V.view((b, L, self.d_v))\n",
    "        \n",
    "        # scale stuff\n",
    "        # k^(1/4) * k^(1/4) = k^(1/2)\n",
    "        Q = Q / self.d_k ** (1/4)\n",
    "        K = K / self.d_k ** (1/4)\n",
    "        \n",
    "        # calculate attention\n",
    "        Z = torch.bmm(Q, K.transpose(1, 2))\n",
    "        A = F.softmax(Z, dim=2) \n",
    "        \n",
    "        # output\n",
    "        Y = torch.bmm(A, V)\n",
    "        \n",
    "        assert Y.shape == torch.Size((b, L, self.d_v))\n",
    "        \n",
    "        return Y, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 2\n",
    "L = 10\n",
    "d = 4\n",
    "d_k = d\n",
    "d_v = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "X = torch.rand((b, L, d))\n",
    "#\n",
    "model = Attention(d=d, d_k=d_k, d_v=d_v)\n",
    "#\n",
    "Y, A = model(X)\n",
    "#\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d:int, d_k=int, d_v=int, heads=int):\n",
    "        super().__init__()\n",
    "        #\n",
    "        self.d = d\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.heads = heads        \n",
    "        #\n",
    "        self.to_K = nn.Linear(d, d_k * heads, bias=False)\n",
    "        self.to_Q = nn.Linear(d, d_k * heads, bias=False)\n",
    "        self.to_V = nn.Linear(d, d_v * heads, bias=False)\n",
    "        #\n",
    "        self.concat_heads = nn.Linear(heads * d_v, d_v)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \n",
    "        b, L, d = X.size()\n",
    "        assert self.d == d\n",
    "        #\n",
    "        Q = self.to_Q(X)\n",
    "        K = self.to_K(X)\n",
    "        V = self.to_V(X)\n",
    "        #\n",
    "        Q = Q.view((b, L, self.heads, self.d_k))\n",
    "        K = K.view((b, L, self.heads, self.d_k))\n",
    "        V = V.view((b, L, self.heads, self.d_v))\n",
    "        #\n",
    "        assert Q.shape == torch.Size((b, L, self.heads, self.d_k))\n",
    "        assert K.shape == torch.Size((b, L, self.heads, self.d_k))\n",
    "        assert V.shape == torch.Size((b, L, self.heads, self.d_v))\n",
    "        #\n",
    "        # reshape (b, L, h, d) to (b, h, L, d)\n",
    "        K = K.transpose(1, 2).contiguous().view(b * self.heads, L, self.d_k)\n",
    "        Q = Q.transpose(1, 2).contiguous().view(b * self.heads, L, self.d_k)\n",
    "        V = V.transpose(1, 2).contiguous().view(b * self.heads, L, self.d_v)\n",
    "        #\n",
    "        # scale stuff\n",
    "        # k^(1/4) * k^(1/4) = k^(1/2)\n",
    "        Q = Q / (self.d_k ** (1/4))\n",
    "        K = K / (self.d_k ** (1/4))\n",
    "        # calculate attention\n",
    "        Z = torch.bmm(Q, K.transpose(1, 2))\n",
    "        #\n",
    "        assert Z.size() == (b*self.heads, L, L)\n",
    "        #\n",
    "        A = F.softmax(Z, dim=2)\n",
    "        \n",
    "        # output\n",
    "        Y = torch.bmm(A, V).view(b, self.heads, L, self.d_v)\n",
    "        \n",
    "        Y = Y.transpose(1, 2).contiguous().view(b, L, self.heads * self.d_v)\n",
    "        Y = self.concat_heads(Y)\n",
    "        return Y, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 1\n",
    "L = 2\n",
    "heads = 3\n",
    "d = 4\n",
    "d_k = d\n",
    "d_v = 5\n",
    "#\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "X = torch.rand((b, L, d))\n",
    "#\n",
    "torch.manual_seed(0)\n",
    "model = MultiHeadAttention(d=d, d_k=d_k, d_v=d_v, heads=heads)\n",
    "Y, A = model(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d:int, heads:int, layer_width=8):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention = MultiHeadAttention(d, d, d, heads)\n",
    "        self.norm1 = nn.LayerNorm(d)\n",
    "        self.norm2 = nn.LayerNorm(d)\n",
    "    \n",
    "        self.mlp = nn.Sequential(\n",
    "              nn.Linear(d, layer_width * d),\n",
    "              nn.ReLU(),\n",
    "              nn.Linear(d * layer_width, d))\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y, _ = self.attention(X)\n",
    "        X = self.norm1(Y + X)\n",
    "        #\n",
    "        Y = self.mlp(X)\n",
    "        X = self.norm2(Y + X)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 1\n",
    "L = 2\n",
    "h = 3\n",
    "d = 4\n",
    "#\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "X = torch.rand((b, L, d))\n",
    "#\n",
    "torch.manual_seed(0)\n",
    "model = TransformerBlock(d, h)\n",
    "Y = model(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "#### word embeddigns\n",
    "#### positional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d:int, heads:int, depth:int, seq_length:int, num_tokens:int, num_classes:int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_tokens = num_tokens\n",
    "        self.token_emb = nn.Embedding(num_tokens, d)\n",
    "        self.pos_emb = nn.Embedding(seq_length, d)\n",
    "\n",
    "        # The sequence of transformer blocks that does all the \n",
    "        # heavy lifting\n",
    "        tblocks = []\n",
    "        for i in range(depth):\n",
    "            tblocks.append(TransformerBlock(d=d, heads=heads))\n",
    "        self.tblocks = nn.Sequential(*tblocks)\n",
    "\n",
    "        # Maps the final output sequence to class logits\n",
    "        self.toprobs = nn.Linear(d, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: A (b, t) tensor of integer values representing \n",
    "                  words (in some predetermined vocabulary).\n",
    "        :return: A (b, c) tensor of log-probabilities over the \n",
    "                 classes (where c is the nr. of classes).\n",
    "        \"\"\"\n",
    "        # generate token embeddings\n",
    "        tokens = self.token_emb(x)\n",
    "        b, t, k = tokens.size()\n",
    "\n",
    "        # generate position embeddings\n",
    "        positions = torch.arange(t, device=DEVICE)\n",
    "        positions = self.pos_emb(positions)[None, :, :].expand(b, t, k)\n",
    "        \n",
    "        x = tokens + positions\n",
    "        x = self.tblocks(x)\n",
    "        \n",
    "        # Average-pool over the t dimension and project to class \n",
    "        # probabilities\n",
    "        x = self.toprobs(x.mean(dim=1))\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(d, 8, 2, 10, 20, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data, datasets, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50_000\n",
    "batch_size = 8\n",
    "epochs = 80\n",
    "lr = 0.0001\n",
    "d = 128\n",
    "max_length = 512 # max sequence length\n",
    "heads = 8\n",
    "depth = 6\n",
    "classes=2\n",
    "path = \"/home/matthias/projects/Transformer/data/checkpoints/model_cp{}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(lower=True, include_lengths=True, batch_first=True)\n",
    "LABEL = data.Field(sequential=False)\n",
    "\n",
    "tdata, _ = datasets.IMDB.splits(TEXT, LABEL)\n",
    "train, test = tdata.split(split_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, max_size=vocab_size - 2) # - 2 to make space for <unk> and <pad>\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "train_iter, test_iter = data.BucketIterator.splits((train, test), batch_size=batch_size, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(d=d, heads=heads, depth=depth, seq_length=max_length, num_tokens=vocab_size, num_classes=classes).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(lr=lr, params=model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen = 0\n",
    "for e in range(epochs):\n",
    "    print(f'\\n epoch {e}')\n",
    "    model.train(True)\n",
    "\n",
    "    for batch in tqdm.tqdm(train_iter):\n",
    "\n",
    "        opt.zero_grad()\n",
    "\n",
    "        input = batch.text[0]\n",
    "        label = batch.label - 1\n",
    "\n",
    "        if input.size(1) > max_length:\n",
    "            input = input[:, :max_length]\n",
    "        out = model(input)\n",
    "        loss = F.nll_loss(out, label)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        \n",
    "        opt.step()\n",
    "        seen += input.size(0)\n",
    "        break ###############################################################\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.train(False)\n",
    "        tot, cor= 0.0, 0.0\n",
    "\n",
    "        for batch in test_iter:\n",
    "            input = batch.text[0]\n",
    "            label = batch.label - 1\n",
    "\n",
    "            if input.size(1) > max_length:\n",
    "                input = input[:, :max_length]\n",
    "            out = model(input).argmax(dim=1)\n",
    "\n",
    "            tot += float(input.size(0))\n",
    "            cor += float((label == out).sum().item())\n",
    "            break #########################################################\n",
    "        acc = cor / tot\n",
    "        print(f'-- \"test\" accuracy {acc:.3}')\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a7be31d1dd9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/home/matthias/projects/Transformer/data/checkpoints/model_cp{}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "path = \"/home/matthias/projects/Transformer/data/checkpoints/model_cp{}\"\n",
    "torch.save(model.state_dict(), path.format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(path.format(e)))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
